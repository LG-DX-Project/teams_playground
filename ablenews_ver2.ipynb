{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d220804-9189-4af1-8bb1-904d98579ded",
   "metadata": {},
   "source": [
    "## ì—ì´ë¸” ë‰´ìŠ¤_ ì˜¤í”¼ë‹ˆì–¸ ver2\n",
    "- https://www.ablenews.co.kr/\n",
    "- íŠ¹ì„±: ì¥ì•  ì „ë°˜ì˜ ê³ ì¸µê³¼ ì‚¬íšŒ ë¬¸ì œê°€ ëŒ“ê¸€/ê¸°ì‚¬ ë§ì´ ë“œëŸ¬ë‚˜ì„œ ë°ì´í„° í’ˆì§ˆë„ ë†’ìŒ.\n",
    "- íƒ€ì¼“ :\n",
    "    - ì˜¤í”¼ë‹ˆì–¸ > ì„¸ìƒì´ì•¼ê¸° > ì •í˜„ì„ì˜ ìì·¨ë°© ì´ì•¼ê¸°\n",
    "    - ì˜¤í”¼ë‹ˆì–¸ > ì„¸ìƒì´ì•¼ê¸° > ì„œì¸í™˜ì˜ íšŒì´ˆë¦¬\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- AbleNews SRN181 / SRN182 í¬ë¡¤ëŸ¬ + 2023ë…„ ì´í›„ í•„í„° + ì›Œë“œí´ë¼ìš°ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5afc60-59dc-43c4-b89d-b3869d886d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰ ì¤‘...\n",
      "ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ!\n",
      "\n",
      "\n",
      "===== ì¹´í…Œê³ ë¦¬ [SRN182] ì²˜ë¦¬ ì‹œì‘ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SRN182] ëª©ë¡ í˜ì´ì§€:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 4/15 [00:26<01:12,  6.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ 5í˜ì´ì§€ì—ì„œ ìƒˆ URL ì—†ìŒ â†’ ì¤‘ë‹¨\n",
      "â¡ ì´ 76ê°œ URL ìˆ˜ì§‘ë¨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SRN182] ë³¸ë¬¸ ìˆ˜ì§‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [02:57<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [SRN182] ìœ íš¨ ë°ì´í„° 41ê±´ â†’ ablenews_result/ablenews_SRN182.csv\n",
      "\n",
      "===== ì¹´í…Œê³ ë¦¬ [SRN181] ì²˜ë¦¬ ì‹œì‘ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SRN181] ëª©ë¡ í˜ì´ì§€: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:33<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¡ ì´ 300ê°œ URL ìˆ˜ì§‘ë¨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SRN181] ë³¸ë¬¸ ìˆ˜ì§‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [12:32<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [SRN181] ìœ íš¨ ë°ì´í„° 289ê±´ â†’ ablenews_result/ablenews_SRN181.csv\n",
      "\n",
      "ğŸ¯ ì „ì²´ í•©ì¹˜ê¸° ì™„ë£Œ â†’ ablenews_result/ablenews_ALL.csv (ì´ 330ê±´)\n",
      "ğŸ’¾ PKL ì €ì¥ ì™„ë£Œ â†’ ablenews_result/ablenews_ALL.pkl\n",
      "\n",
      "ğŸ” TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...\n",
      "ğŸ¯ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!\n",
      "       word     tfidf\n",
      "2093    ì„œë¹„ìŠ¤  0.041253\n",
      "3513     ì¥ì•   0.039696\n",
      "4007     ì§€ì›  0.032050\n",
      "1923     ì‚¬ëŒ  0.029423\n",
      "2405  ì‹œê°ì¥ì• ì¸  0.028455\n",
      "471      êµìœ¡  0.026911\n",
      "3246    ì´ì•¼ê¸°  0.026578\n",
      "4921     í™œë™  0.026339\n",
      "1974     ì‚¬íšŒ  0.025836\n",
      "2433     ì‹œì„¤  0.024825\n",
      "251      ê²½ìš°  0.024813\n",
      "676      ê¸°ìˆ   0.024341\n",
      "1949     ì‚¬ì—…  0.022518\n",
      "3472     ì‘ê°€  0.022143\n",
      "3250     ì´ìš©  0.021555\n",
      "1952     ì‚¬ìš©  0.021429\n",
      "1447     ë¬¸ì œ  0.021353\n",
      "2406     ì‹œê°„  0.020682\n",
      "3306     ì¸ì‹  0.020066\n",
      "135      ê°œë°œ  0.019484\n",
      "1759     ë³µì§€  0.019276\n",
      "1007     ëŒ€í•œ  0.018843\n",
      "2063     ìƒê°  0.018124\n",
      "3445     ìì‹   0.017990\n",
      "42       ê°€ì¡±  0.017966\n",
      "3662    ì ‘ê·¼ì„±  0.017888\n",
      "1625     ë²„ìŠ¤  0.017792\n",
      "903      ë‹¨ì²´  0.017423\n",
      "1168    ë””ì§€í„¸  0.017232\n",
      "3684     ì •ë³´  0.017190\n",
      "1554   ë°œë‹¬ì¥ì•   0.017059\n",
      "646      ê¸°ê´€  0.016908\n",
      "569      ê¶Œë¦¬  0.016447\n",
      "4704     í•™êµ  0.016086\n",
      "372      ê³µì—°  0.016074\n",
      "44       ê°€ì§€  0.015948\n",
      "140      ê°œì„   0.015934\n",
      "3741     ì œê³µ  0.015806\n",
      "4100     ì°¨ë³„  0.015802\n",
      "4974    íœ ì²´ì–´  0.015612\n",
      "3778     ì œí’ˆ  0.015449\n",
      "866      ë‹¤ë¥¸  0.015094\n",
      "608      ê·¸ë¦¼  0.014662\n",
      "2596     ì•„ì´  0.014590\n",
      "4428     íƒì‹œ  0.014479\n",
      "2625     ì•ˆë‚´  0.014255\n",
      "4124     ì°¸ì—¬  0.014219\n",
      "3657     ì ì  0.014141\n",
      "1788    ë¶€ëª¨ë‹˜  0.014002\n",
      "2154     ì„¤ì¹˜  0.013793\n",
      "\n",
      "ğŸ‰ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import time, os, traceback, re\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver as wb\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1. ì´ˆê¸° ì„¤ì •\n",
    "# ================================================================\n",
    "CATEGORY_CODES = [\"SRN182\", \"SRN181\"]\n",
    "\n",
    "MIN_YEAR = 2022\n",
    "MIN_DATE = datetime(MIN_YEAR, 1, 1)\n",
    "\n",
    "op_url = 'https://www.ablenews.co.kr/news/articleList.html?sc_sub_section_code=S2N3&view_type=sm'\n",
    "\n",
    "BASE_LIST_URL = (\n",
    "    \"https://www.ablenews.co.kr/news/articleList.html\"\n",
    "    \"?sc_serial_code={code}&sc_sub_section_code=S2N3&sc_section_code=S1N2\"\n",
    "    \"&view_type=sm&page={page}\"\n",
    ")\n",
    "\n",
    "MAX_PAGE = 15\n",
    "KOREAN_FONT_PATH = \"/System/Library/Fonts/AppleSDGothicNeo.ttc\"\n",
    "\n",
    "out_dir = \"ablenews_result\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "error_log_file = os.path.join(out_dir, \"ablenews_crawl_errors.log\")\n",
    "open(error_log_file, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2. ì—ëŸ¬ ë¡œê·¸ í•¨ìˆ˜\n",
    "# ================================================================\n",
    "def log_error(stage, msg=\"\", cat=None, url=None, exc: Exception | None = None):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    lines = [f\"[{ts}] [{stage}]\"]\n",
    "    if cat:\n",
    "        lines.append(f\"category={cat}\")\n",
    "    if url:\n",
    "        lines.append(f\"url={url}\")\n",
    "    if msg:\n",
    "        lines.append(f\"msg={msg}\")\n",
    "    if exc:\n",
    "        lines.append(\"traceback:\")\n",
    "        lines.append(traceback.format_exc())\n",
    "\n",
    "    with open(error_log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" | \".join(lines) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3. ë‚ ì§œ íŒŒì‹±\n",
    "# ================================================================\n",
    "def parse_article_date(date_text: str):\n",
    "    if not date_text:\n",
    "        return None\n",
    "\n",
    "    txt = date_text.strip()\n",
    "    for prefix in [\"ì…ë ¥\", \"ë“±ë¡\", \"ìˆ˜ì •\"]:\n",
    "        if txt.startswith(prefix):\n",
    "            txt = txt[len(prefix):].strip()\n",
    "            break\n",
    "\n",
    "    txt = txt.replace(\"Â·\", \" \")\n",
    "    txt = txt.replace(\"ë…„\", \"-\").replace(\"ì›”\", \"-\").replace(\"ì¼\", \"\")\n",
    "    txt = txt.replace(\".\", \"-\").strip()\n",
    "\n",
    "    for fmt in [\"%Y-%m-%d %H:%M\", \"%Y-%m-%d\"]:\n",
    "        try:\n",
    "            return datetime.strptime(txt, fmt)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4. í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰\n",
    "# ================================================================\n",
    "print(\"ì›¹ë“œë¼ì´ë²„ ì‹¤í–‰ ì¤‘...\")\n",
    "driver = wb.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(op_url)\n",
    "print(\"ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ!\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5. URL ìˆ˜ì§‘ + ë³¸ë¬¸ ìˆ˜ì§‘\n",
    "# ================================================================\n",
    "all_frames = []\n",
    "\n",
    "for cat_code in CATEGORY_CODES:\n",
    "    print(f\"\\n===== ì¹´í…Œê³ ë¦¬ [{cat_code}] ì²˜ë¦¬ ì‹œì‘ =====\")\n",
    "    page_url_list = []\n",
    "\n",
    "    try:\n",
    "        for page in tqdm(range(1, MAX_PAGE + 1), desc=f\"[{cat_code}] ëª©ë¡ í˜ì´ì§€\"):\n",
    "            list_url = BASE_LIST_URL.format(code=cat_code, page=page)\n",
    "            driver.get(list_url)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            article_links = driver.find_elements(By.CSS_SELECTOR,\n",
    "                \"#section-list ul.type2 li div.view-cont h4.titles a\")\n",
    "\n",
    "            new_cnt = 0\n",
    "            for a in article_links:\n",
    "                href = a.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    page_url_list.append(href)\n",
    "                    new_cnt += 1\n",
    "\n",
    "            if new_cnt == 0:\n",
    "                print(f\"âš ï¸ {page}í˜ì´ì§€ì—ì„œ ìƒˆ URL ì—†ìŒ â†’ ì¤‘ë‹¨\")\n",
    "                break\n",
    "\n",
    "        page_url_list = list(dict.fromkeys(page_url_list))\n",
    "        print(f\"â¡ ì´ {len(page_url_list)}ê°œ URL ìˆ˜ì§‘ë¨\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(\"URL ìˆ˜ì§‘\", \"ì˜¤ë¥˜\", cat=cat_code, exc=e)\n",
    "\n",
    "    # ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘\n",
    "    df_rows = []\n",
    "    for href in tqdm(page_url_list, desc=f\"[{cat_code}] ë³¸ë¬¸ ìˆ˜ì§‘\"):\n",
    "        try:\n",
    "            driver.get(href)\n",
    "            time.sleep(1.2)\n",
    "\n",
    "            try:\n",
    "                title = driver.find_element(By.CSS_SELECTOR,\n",
    "                    \"#articleViewCon > article.grid.body > header > h3\").text.strip()\n",
    "            except:\n",
    "                title = None\n",
    "\n",
    "            try:\n",
    "                date_raw = driver.find_element(By.CSS_SELECTOR,\n",
    "                    \"#articleViewCon > article.grid.body > header > ul > li:nth-child(2)\").text.strip()\n",
    "            except:\n",
    "                date_raw = None\n",
    "\n",
    "            try:\n",
    "                content = driver.find_element(By.CSS_SELECTOR,\n",
    "                    \"#article-view-content-div\").text.strip()\n",
    "            except:\n",
    "                content = None\n",
    "\n",
    "            parsed_date = parse_article_date(date_raw)\n",
    "\n",
    "            if parsed_date and parsed_date < MIN_DATE:\n",
    "                continue\n",
    "\n",
    "            df_rows.append([cat_code, title, content, date_raw, parsed_date, href])\n",
    "\n",
    "        except Exception as e:\n",
    "            log_error(\"ë³¸ë¬¸ í¬ë¡¤ë§\", \"ì˜¤ë¥˜\", cat=cat_code, url=href, exc=e)\n",
    "\n",
    "    col_names = [\"category\", \"title\", \"content\", \"date_raw\", \"date_parsed\", \"url\"]\n",
    "    cat_df = pd.DataFrame(df_rows, columns=col_names)\n",
    "\n",
    "    cat_df_clean = cat_df.dropna(subset=[\"title\", \"content\", \"date_raw\"])\n",
    "    cat_df_clean = cat_df_clean.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "    if not cat_df_clean.empty:\n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ CSV ì €ì¥\n",
    "        per_cat_csv = os.path.join(out_dir, f\"ablenews_{cat_code}.csv\")\n",
    "        cat_df_clean.to_csv(per_cat_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… [{cat_code}] ìœ íš¨ ë°ì´í„° {len(cat_df_clean)}ê±´ â†’ {per_cat_csv}\")\n",
    "        all_frames.append(cat_df_clean)\n",
    "    else:\n",
    "        log_error(stage=\"CSV ì €ì¥\", msg=\"ìœ íš¨ ë°ì´í„° ì—†ìŒ â†’ CSV ë¯¸ìƒì„±\", cat=cat_code)\n",
    "        print(f\"âš ï¸ [{cat_code}] ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6. ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸° + CSV, PKL ì €ì¥\n",
    "# ================================================================\n",
    "if all_frames:\n",
    "    total_df = pd.concat(all_frames, ignore_index=True)\n",
    "    total_df = total_df.drop_duplicates(subset=[\"url\"]).copy()\n",
    "\n",
    "    total_csv = os.path.join(out_dir, \"ablenews_ALL.csv\")\n",
    "    total_pkl = os.path.join(out_dir, \"ablenews_ALL.pkl\")\n",
    "\n",
    "    total_df.to_csv(total_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    total_df.to_pickle(total_pkl)\n",
    "\n",
    "    print(f\"\\nğŸ¯ ì „ì²´ í•©ì¹˜ê¸° ì™„ë£Œ â†’ {total_csv} (ì´ {len(total_df)}ê±´)\")\n",
    "    print(f\"ğŸ’¾ PKL ì €ì¥ ì™„ë£Œ â†’ {total_pkl}\")\n",
    "\n",
    "else:\n",
    "    log_error(stage=\"ìµœì¢… í•©ì¹˜ê¸°\", msg=\"í•©ì¹  ë°ì´í„° ì—†ìŒ\")\n",
    "    print(\"\\nâš ï¸ ì „ì²´ í•©ì¹  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    total_df = pd.DataFrame(columns=[\"category\",\"title\",\"content\",\"date_raw\",\"date_parsed\",\"url\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 7. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ / í’ˆì‚¬ ì¶”ì¶œ\n",
    "# ================================================================\n",
    "okt = Okt()\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "    text = re.sub(r'[^ê°€-í£\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_korean_nouns(texts):\n",
    "    nouns = []\n",
    "    for t in texts:\n",
    "        cleaned = clean_text(t)\n",
    "        pos = okt.pos(cleaned, norm=True, stem=True)\n",
    "        for word, tag in pos:\n",
    "            if tag == \"Noun\":\n",
    "                nouns.append(word)\n",
    "    return \" \".join(nouns)\n",
    "\n",
    "def extract_korean_adjectives(texts):\n",
    "    adjs = []\n",
    "    for t in texts:\n",
    "        cleaned = clean_text(t)\n",
    "        pos = okt.pos(cleaned, norm=True, stem=True)\n",
    "        for word, tag in pos:\n",
    "            if tag == \"Adjective\":\n",
    "                adjs.append(word)\n",
    "    return \" \".join(adjs)\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 8. TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ  â† ìˆœì„œ ë¨¼ì €!\n",
    "# ================================================================\n",
    "def extract_keywords_tfidf(df, top_n=50):\n",
    "    print(\"\\nğŸ” TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...\")\n",
    "\n",
    "    texts = df[\"content\"].dropna().astype(str).tolist()\n",
    "    corpus = [extract_korean_nouns([t]) for t in texts]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.8)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    tfidf_scores = np.asarray(X.mean(axis=0)).ravel()\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_df = pd.DataFrame({\n",
    "        \"word\": vocab,\n",
    "        \"tfidf\": tfidf_scores\n",
    "    })\n",
    "\n",
    "    top_keywords = tfidf_df.sort_values(\"tfidf\", ascending=False).head(top_n)\n",
    "\n",
    "    top_keywords.to_csv(os.path.join(out_dir, \"ablenews_tfidf_keywords.csv\"),\n",
    "                        index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"ğŸ¯ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "    print(top_keywords)\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "tfidf_keywords = extract_keywords_tfidf(total_df, top_n=50)\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 9. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "# ================================================================\n",
    "def create_wordclouds(df):\n",
    "    texts = df[\"content\"].dropna().astype(str).tolist()\n",
    "\n",
    "    nouns_text = extract_korean_nouns(texts)\n",
    "    adjs_text = extract_korean_adjectives(texts)\n",
    "\n",
    "    wc_noun = WordCloud(\n",
    "        font_path=KOREAN_FONT_PATH,\n",
    "        width=1600, height=900,\n",
    "        background_color=\"white\"\n",
    "    ).generate(nouns_text)\n",
    "    wc_noun.to_file(os.path.join(out_dir, \"ablenews_wordcloud_nouns.png\"))\n",
    "\n",
    "    wc_adj = WordCloud(\n",
    "        font_path=KOREAN_FONT_PATH,\n",
    "        width=1600, height=900,\n",
    "        background_color=\"white\"\n",
    "    ).generate(adjs_text)\n",
    "    wc_adj.to_file(os.path.join(out_dir, \"ablenews_wordcloud_adjectives.png\"))\n",
    "\n",
    "    print(\"\\nğŸ‰ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "\n",
    "create_wordclouds(total_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96c355-b771-4a9b-b440-30a3d59b42f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f92aca5-bd20-42fc-80c2-e0611bd7a682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46289cf7-115d-423b-81fa-43aa913acd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8085c93-47a5-4bba-a0ff-7d40c0c23018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046928fe-cf1f-446b-b0f3-03fd251b14e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f3497-bbca-4e34-b5c4-ad11ee17c426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (web-crawling)",
   "language": "python",
   "name": "web-crawling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

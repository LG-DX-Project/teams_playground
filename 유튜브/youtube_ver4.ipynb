{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fcd791-4170-485a-b469-f2a7ffe2a4c8",
   "metadata": {},
   "source": [
    "### ìœ íŠœë¸Œ ì˜ìƒ ëŒ“ê¸€ í¬ë¡¤ë§ ver3\n",
    "- ì˜ìƒ ìë§‰ ìŠ¤í¬ë¦½íŠ¸ + ëŒ“ê¸€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5099b-3571-4047-ab82-7aca9d3e7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######### ì‹ ë²„ì „ ##########\n",
    "# from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# video_id = \"lxZsn3cwdAU\"\n",
    "\n",
    "# api = YouTubeTranscriptApi()\n",
    "# transcription = api.fetch(video_id, languages=['ko', 'en'])\n",
    "\n",
    "# for seg in transcription:\n",
    "#     print(seg.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e7458-82f7-4c25-a3bd-66e1863a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "import os\n",
    "import re\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# ================================\n",
    "# 0) API í‚¤ & ìˆ˜ì§‘í•  URL ì„¤ì •\n",
    "# ================================\n",
    "API_KEY = \"\"  # ğŸ”¹ ì—¬ê¸°ì— YouTube Data API í‚¤ ë„£ê¸°\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/shorts/lxZsn3cwdAU\",\n",
    "    \"https://www.youtube.com/watch?v=dFRXcYJVfMM\",\n",
    "]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ í´ë” ì„¤ì •\n",
    "RESULT_DIR = \"youtube_results_commentsADDscript\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1) URLì—ì„œ video_id ì¶”ì¶œ í•¨ìˆ˜\n",
    "# ================================\n",
    "def extract_video_id(url: str):\n",
    "    \"\"\"\n",
    "    ìœ íŠœë¸Œ ì¼ë°˜ ì˜ìƒ & ì‡¼ì¸  URLì—ì„œ video_idë§Œ ë½‘ì•„ì£¼ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    # ì¿¼ë¦¬íŒŒë¼ë¯¸í„°(?) ì œê±°ìš©\n",
    "    base = url.split(\"?\")[0]\n",
    "\n",
    "    # shorts í˜•ì‹: https://www.youtube.com/shorts/VIDEO_ID\n",
    "    if \"shorts/\" in base:\n",
    "        return base.split(\"shorts/\")[1]\n",
    "\n",
    "    # ì¼ë°˜ ì˜ìƒ í˜•ì‹: https://www.youtube.com/watch?v=VIDEO_ID\n",
    "    if \"watch\" in base and \"v=\" in url:\n",
    "        return url.split(\"v=\")[1].split(\"&\")[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2) íŠ¹ì • video_id ì „ì²´ ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "# ================================\n",
    "def fetch_all_comments_for_video(video_id: str, sleep_sec: float = 0.2):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • video_idì— ëŒ€í•œ ìƒìœ„ ëŒ“ê¸€(Top-level comments)ì„\n",
    "    ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ ì „ë¶€ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"videoId\": video_id,\n",
    "        \"maxResults\": 100,           # í•œ í˜ì´ì§€ ìµœëŒ€ 100ê°œ\n",
    "        \"textFormat\": \"plainText\",\n",
    "        \"key\": API_KEY,\n",
    "    }\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    page_idx = 0\n",
    "\n",
    "    while True:\n",
    "        if next_page_token:\n",
    "            params[\"pageToken\"] = next_page_token\n",
    "\n",
    "        res = requests.get(url, params=params)\n",
    "        data = res.json()\n",
    "\n",
    "        # ì—ëŸ¬ ë°œìƒ ì‹œ ì¶œë ¥í•˜ê³  ì¤‘ë‹¨\n",
    "        if \"error\" in data:\n",
    "            print(f\"âš ï¸ API Error for video {video_id}: \", data[\"error\"])\n",
    "            break\n",
    "\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comments.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"source\": \"comment\",\n",
    "                \"comment_id\": item[\"id\"],\n",
    "                \"author\": snippet.get(\"authorDisplayName\"),\n",
    "                \"text\": snippet.get(\"textDisplay\"),\n",
    "                \"likeCount\": snippet.get(\"likeCount\"),\n",
    "                \"publishedAt\": snippet.get(\"publishedAt\"),\n",
    "            })\n",
    "\n",
    "        page_idx += 1\n",
    "        print(f\"âœ… {video_id} - {page_idx} í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ, ëˆ„ì  ëŒ“ê¸€ ìˆ˜: {len(comments)}\")\n",
    "\n",
    "        # ë‹¤ìŒ í˜ì´ì§€ í† í° í™•ì¸\n",
    "        next_page_token = data.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        sleep(sleep_sec)  # API í˜¸ì¶œ ê°„ ì‚´ì§ ì‰¬ì–´ì£¼ê¸° (ì¿¼í„°/ì°¨ë‹¨ ë°©ì§€)\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2-1) íŠ¹ì • video_id ìŠ¤í¬ë¦½íŠ¸(ìë§‰) ìˆ˜ì§‘\n",
    "# ================================\n",
    "def fetch_full_transcript(video_id: str, languages=None) -> str | None:\n",
    "    \"\"\"\n",
    "    youtube_transcript_apiì˜ ìµœì‹  API (api.fetch) ë°©ì‹ìœ¼ë¡œ\n",
    "    ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    if languages is None:\n",
    "        languages = ['ko', 'en']  # í•œêµ­ì–´ > ì˜ì–´ ìš°ì„ \n",
    "\n",
    "    try:\n",
    "        api = YouTubeTranscriptApi()\n",
    "\n",
    "        # ğŸ”¹ ì‹ ë²„ì „: fetch() ì‚¬ìš©\n",
    "        segments = api.fetch(video_id, languages=languages)\n",
    "\n",
    "        # ğŸ”¹ ìë§‰ í…ìŠ¤íŠ¸ë§Œ í•©ì¹˜ê¸°\n",
    "        full_text = \" \".join(seg.get(\"text\", \"\") for seg in segments if seg.get(\"text\"))\n",
    "\n",
    "        print(f\"ğŸ“œ {video_id} ìŠ¤í¬ë¦½íŠ¸ ê¸¸ì´: {len(full_text)}ì\")\n",
    "\n",
    "        if not full_text.strip():\n",
    "            print(f\"âš ï¸ {video_id} ìŠ¤í¬ë¦½íŠ¸ ë¹„ì–´ìˆìŒ\")\n",
    "            return None\n",
    "\n",
    "        return full_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Transcript fetch ì‹¤íŒ¨ ({video_id}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3) ëª¨ë“  URLì— ëŒ€í•´ ìŠ¤í¬ë¦½íŠ¸ + ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "# ================================\n",
    "def collect_data_from_urls(url_list):\n",
    "    video_ids = [extract_video_id(u) for u in url_list]\n",
    "    print(\"ğŸ¬ ì¶”ì¶œëœ video_id ëª©ë¡:\", video_ids)\n",
    "\n",
    "    all_rows = []  # ì „ì²´ ì˜ìƒ í†µí•©ìš©\n",
    "\n",
    "    for vid in video_ids:\n",
    "        if vid is None:\n",
    "            print(f\"âš ï¸ URLì—ì„œ video_idë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ“¥ {vid} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\")\n",
    "\n",
    "        per_video_rows = []  # ì´ ì˜ìƒ í•œ ê°œì— ëŒ€í•œ í–‰ë“¤ë§Œ\n",
    "\n",
    "        # 1) ìŠ¤í¬ë¦½íŠ¸(ìë§‰) ìˆ˜ì§‘\n",
    "        script_text = fetch_full_transcript(vid)\n",
    "        if script_text:\n",
    "            script_row = {\n",
    "                \"video_id\": vid,\n",
    "                \"source\": \"script\",   # ğŸ”¹ ìŠ¤í¬ë¦½íŠ¸\n",
    "                \"comment_id\": None,\n",
    "                \"author\": None,\n",
    "                \"text\": script_text,\n",
    "                \"likeCount\": None,\n",
    "                \"publishedAt\": None,\n",
    "            }\n",
    "            per_video_rows.append(script_row)\n",
    "            all_rows.append(script_row)\n",
    "            print(f\"âœ… {vid} ìŠ¤í¬ë¦½íŠ¸ 1í–‰ ì¶”ê°€ ì™„ë£Œ\")\n",
    "\n",
    "        # 2) ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "        print(f\"ğŸ’¬ {vid} ëŒ“ê¸€ ì „ì²´ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "        video_comments = fetch_all_comments_for_video(vid)\n",
    "\n",
    "        if video_comments:\n",
    "            # ì´ë¯¸ source, video_id, text ë“± í¬í•¨ë˜ì–´ ìˆìŒ\n",
    "            per_video_rows.extend(video_comments)\n",
    "            all_rows.extend(video_comments)\n",
    "            print(f\"âœ… {vid} ëŒ“ê¸€ {len(video_comments)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ ë™ì˜ìƒ {vid} ì—ëŠ” ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # 3) ì´ ì˜ìƒ ì „ìš© CSV ì €ì¥ (ìŠ¤í¬ë¦½íŠ¸ + ëŒ“ê¸€)\n",
    "        if per_video_rows:\n",
    "            df_vid = pd.DataFrame(per_video_rows)\n",
    "            filename = f\"video_{vid}_script_comments.csv\"\n",
    "            path = os.path.join(RESULT_DIR, filename)\n",
    "            df_vid.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"ğŸ’¾ (ì˜ìƒë³„ ì €ì¥) ë™ì˜ìƒ {vid} CSV ì €ì¥ ì™„ë£Œ: {path}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ ë™ì˜ìƒ {vid} ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ëª¨ë“  ì˜ìƒ í•©ì¹œ DataFrame\n",
    "    df_all = pd.DataFrame(all_rows)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4) CSV & PKL ì €ì¥ (ì „ì²´ í†µí•©)\n",
    "# ================================\n",
    "def save_comments(df: pd.DataFrame,\n",
    "                  csv_name=\"youtube_all_videos_script_comments.csv\",\n",
    "                  pkl_name=\"youtube_all_videos_script_comments.pkl\"):\n",
    "    csv_path = os.path.join(RESULT_DIR, csv_name)\n",
    "    pkl_path = os.path.join(RESULT_DIR, pkl_name)\n",
    "\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    df.to_pickle(pkl_path)\n",
    "\n",
    "    print(f\"ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: {csv_path}\")\n",
    "    print(f\"ğŸ’¾ PKL ì €ì¥ ì™„ë£Œ: {pkl_path}\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4-2) í•œêµ­ì–´ ì „ì²˜ë¦¬ + ëª…ì‚¬/í˜•ìš©ì‚¬ ì¶”ì¶œ\n",
    "# ================================\n",
    "okt = Okt()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ê¸°ë³¸ ì „ì²˜ë¦¬:\n",
    "    - URL ì œê±°\n",
    "    - ì´ëª¨ì§€ ì œê±°\n",
    "    - íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    - ì˜ì–´/ìˆ«ì ì œê±°\n",
    "    - ì¤‘ë³µ ê³µë°± ì •ë¦¬\n",
    "    \"\"\"\n",
    "    # 1) URL ì œê±°\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # 2) ì´ëª¨ì§€ ì œê±° (ìœ ë‹ˆì½”ë“œ ìƒìœ„ ì˜ì—­ ë‚ ë¦¬ê¸°)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "\n",
    "    # 3) íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ ë‚¨ê¸°ê¸°)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "    # 4) ì˜ì–´/ìˆ«ì ì œê±° (í•œêµ­ì–´ ìœ„ì£¼ ë¶„ì„ì´ë©´ ì¶”ì²œ)\n",
    "    text = re.sub(r'[a-zA-Z0-9]', ' ', text)\n",
    "\n",
    "    # 5) ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_korean_adjectives(texts):\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ í˜•ìš©ì‚¬(Adjective)ë§Œ ë½‘ì•„ì„œ\n",
    "    í•˜ë‚˜ì˜ í° ë¬¸ìì—´ë¡œ ë°˜í™˜ (ì „ì²˜ë¦¬ í¬í•¨)\n",
    "    \"\"\"\n",
    "    adjectives = []\n",
    "\n",
    "    for t in texts:\n",
    "        try:\n",
    "            cleaned = clean_text(t)  # ğŸ”¥ ì „ì²˜ë¦¬ ë¨¼ì €!\n",
    "            pos_tags = okt.pos(cleaned, norm=True, stem=True)\n",
    "            for word, tag in pos_tags:\n",
    "                if tag == \"Adjective\":\n",
    "                    adjectives.append(word)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return \" \".join(adjectives)\n",
    "\n",
    "\n",
    "def extract_korean_nouns(texts):\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬(Noun)ë§Œ ì¶”ì¶œí•˜ì—¬\n",
    "    í•˜ë‚˜ì˜ í° ë¬¸ìì—´ë¡œ ë°˜í™˜ (ì „ì²˜ë¦¬ í¬í•¨)\n",
    "    \"\"\"\n",
    "    nouns = []\n",
    "\n",
    "    for t in texts:\n",
    "        try:\n",
    "            cleaned = clean_text(t)  # ğŸ”¥ ì „ì²˜ë¦¬ ë¨¼ì €\n",
    "            pos_tags = okt.pos(cleaned, norm=True, stem=True)\n",
    "            for word, tag in pos_tags:\n",
    "                if tag == \"Noun\":\n",
    "                    nouns.append(word)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return \" \".join(nouns)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 5) ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± (ìŠ¤í¬ë¦½íŠ¸+ëŒ“ê¸€ ì „ì²´ ëŒ€ìƒ)\n",
    "# ================================\n",
    "def create_wordcloud(df: pd.DataFrame,\n",
    "                     output_img: str = \"youtube_comments_wordcloud.png\",\n",
    "                     mask_path: str | None = None):\n",
    "    # 1) í…ìŠ¤íŠ¸ ëª¨ìœ¼ê¸° (ìŠ¤í¬ë¦½íŠ¸ + ëŒ“ê¸€ ëª¨ë‘)\n",
    "    texts = df[\"text\"].dropna().astype(str).tolist()\n",
    "    if not texts:\n",
    "        print(\"âš ï¸ ìƒì„±í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # ğŸ”¹ ëª…ì‚¬ / í˜•ìš©ì‚¬ ê°ê° ì „ì²˜ë¦¬ ë° ì¶”ì¶œ\n",
    "    print(\"ğŸ” ëª…ì‚¬ ì¶”ì¶œ ì¤‘...\")\n",
    "    nouns_text = extract_korean_nouns(texts)\n",
    "\n",
    "    print(\"ğŸ” í˜•ìš©ì‚¬ ì¶”ì¶œ ì¤‘...\")\n",
    "    adjs_text = extract_korean_adjectives(texts)\n",
    "\n",
    "    # 3) ë§ˆìŠ¤í¬ ì ìš©\n",
    "    mask = None\n",
    "    if mask_path is not None and os.path.exists(mask_path):\n",
    "        mask = np.array(plt.imread(mask_path))\n",
    "        print(f\"ğŸ­ ë§ˆìŠ¤í¬ ì ìš©: {mask_path}\")\n",
    "    elif mask_path is not None:\n",
    "        print(f\"âš ï¸ ë§ˆìŠ¤í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mask_path} (ë§ˆìŠ¤í¬ ì—†ì´ ìƒì„±)\")\n",
    "\n",
    "    # 4) í°íŠ¸ ê²½ë¡œ ì„¤ì • (í™˜ê²½ ë§ê²Œ ìˆ˜ì •!)\n",
    "    #   - Mac: '/Library/Fonts/AppleGothic.ttf'\n",
    "    #   - Win: 'C:/Windows/Fonts/malgun.ttf'\n",
    "    font_path = \"/Library/Fonts/AppleGothic.ttf\"\n",
    "\n",
    "    # =======\n",
    "    # 1) ëª…ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "    # ======\n",
    "    wc_noun = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        background_color=\"white\",\n",
    "        mask=mask\n",
    "    ).generate(nouns_text)\n",
    "\n",
    "    noun_img_path = os.path.join(RESULT_DIR, \"wordcloud_nouns.png\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wc_noun)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(noun_img_path, dpi=300)\n",
    "    print(f\"ğŸ–¼ ëª…ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ: {noun_img_path}\")\n",
    "\n",
    "    # ========\n",
    "    # 2) í˜•ìš©ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "    # ========\n",
    "    wc_adj = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        background_color=\"white\",\n",
    "        mask=mask\n",
    "    ).generate(adjs_text)\n",
    "\n",
    "    adj_img_path = os.path.join(RESULT_DIR, \"wordcloud_adjectives.png\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wc_adj)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(adj_img_path, dpi=300)\n",
    "    print(f\"ğŸ–¼ í˜•ìš©ì‚¬ ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ: {adj_img_path}\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 6) ë©”ì¸ ì‹¤í–‰ë¶€\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) ìŠ¤í¬ë¦½íŠ¸ + ëŒ“ê¸€ ì „ì²´ ìˆ˜ì§‘\n",
    "    df_all = collect_data_from_urls(urls)\n",
    "    print(\"\\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(df_all.head())\n",
    "\n",
    "    print(\"------------csv, pkl, wordcloud--------------\")\n",
    "    # 2) CSV / PKL ì €ì¥ (ëª¨ë“  ì˜ìƒ í†µí•©)\n",
    "    save_comments(df_all)\n",
    "\n",
    "    # 3) ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (ìŠ¤í¬ë¦½íŠ¸ + ëŒ“ê¸€ ì „ì²´ ëŒ€ìƒ)\n",
    "    create_wordcloud(\n",
    "        df_all,\n",
    "        mask_path=\"heart.jpg\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7e448-d4ab-487c-9262-9e09d061f7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (web-crawling)",
   "language": "python",
   "name": "web-crawling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
